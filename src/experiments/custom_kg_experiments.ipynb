{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b6958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bosa/wn-project/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for knowledge graph embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from pykeen import predict\n",
    "from pykeen.models import TransE, ComplEx, DistMult, RotatE\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.evaluation import RankBasedEvaluator\n",
    "from pykeen.training import SLCWATrainingLoop\n",
    "from pykeen.sampling import BasicNegativeSampler\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Custom function to load triples from our dataset folders\n",
    "def load_custom_dataset(dataset_name):\n",
    "    \"\"\"\n",
    "    Load custom dataset triples from the given folder\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset folder (e.g., 'appdia', 'imdb', etc.)\n",
    "    \n",
    "    Returns:\n",
    "        A TriplesFactory object with the dataset loaded\n",
    "    \"\"\"\n",
    "    # Path to the triples file\n",
    "    triples_path = f\"/home/bosa/skg/data/skg/{dataset_name}/triples.tsv\"\n",
    "    \n",
    "    # Read triples\n",
    "    triples_df = pd.read_csv(triples_path, sep='\\t')\n",
    "    \n",
    "    # Create entity and relation mappings\n",
    "    entity_to_id = {}\n",
    "    relation_to_id = {}\n",
    "    \n",
    "    # First pass to create mappings\n",
    "    for h, r, t in triples_df.values:\n",
    "        if h not in entity_to_id:\n",
    "            entity_to_id[h] = len(entity_to_id)\n",
    "        if t not in entity_to_id:\n",
    "            entity_to_id[t] = len(entity_to_id)\n",
    "        if r not in relation_to_id:\n",
    "            relation_to_id[r] = len(relation_to_id)\n",
    "    \n",
    "    # Second pass to create mapped triples\n",
    "    mapped_triples = []\n",
    "    for h, r, t in triples_df.values:\n",
    "        mapped_triples.append([entity_to_id[h], relation_to_id[r], entity_to_id[t]])\n",
    "    \n",
    "    # Create TriplesFactory\n",
    "    return TriplesFactory(\n",
    "        mapped_triples=np.array(mapped_triples),\n",
    "        entity_to_id=entity_to_id,\n",
    "        relation_to_id=relation_to_id\n",
    "    )\n",
    "\n",
    "# Function to run experiment and print results\n",
    "def run_experiment(dataset_name, model_class, embedding_dim=128, num_epochs=200, batch_size=512, learning_rate=0.01, use_graph_factory=False, graph_factory=None):\n",
    "    \"\"\"\n",
    "    Run a knowledge graph embedding experiment on the given dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset folder\n",
    "        model_class: PyKEEN model class to use\n",
    "        embedding_dim: Dimension of embeddings\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        use_graph_factory: Whether to use the provided graph_factory instead of loading from file\n",
    "        graph_factory: TriplesFactory created from the NetworkX graph\n",
    "    \"\"\"\n",
    "    print(f\"Running experiment with {model_class.__name__} on {dataset_name} dataset\")\n",
    "    \n",
    "    # Load dataset\n",
    "    if use_graph_factory and graph_factory is not None:\n",
    "        print(\"Using graph created with create_nx_graph instead of loading from triples.tsv\")\n",
    "        triples_factory = graph_factory\n",
    "    else:\n",
    "        print(\"Loading triples from triples.tsv file\")\n",
    "        triples_factory = load_custom_dataset(dataset_name)\n",
    "    \n",
    "    # Create train/test split (80/20)\n",
    "    training, testing = triples_factory.split([0.8, 0.2])\n",
    "    \n",
    "    # Create model\n",
    "    model = model_class(triples_factory=training, embedding_dim=embedding_dim)\n",
    "    \n",
    "    # Create optimizer and training loop\n",
    "    optimizer = Adam(params=model.get_grad_params(), lr=learning_rate)\n",
    "    trainer = SLCWATrainingLoop(\n",
    "        model=model,\n",
    "        triples_factory=training,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    \n",
    "    # Run pipeline\n",
    "    result = pipeline(\n",
    "        training=training,\n",
    "        testing=testing,\n",
    "        model=model,\n",
    "        evaluator=RankBasedEvaluator,\n",
    "        negative_sampler=BasicNegativeSampler, \n",
    "        training_loop=trainer,\n",
    "        training_kwargs=dict(\n",
    "            num_epochs=num_epochs,\n",
    "            batch_size=batch_size\n",
    "        ),\n",
    "        evaluator_kwargs=dict(\n",
    "            batch_size=batch_size\n",
    "        ),\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Print metrics\n",
    "    hits10 = result.get_metric('hits_at_10')\n",
    "    hits1 = result.get_metric('hits_at_1')\n",
    "    hits3 = result.get_metric('hits_at_3')\n",
    "    mrr = result.get_metric('mean_reciprocal_rank')\n",
    "    mr = result.get_metric('mean_rank')\n",
    "    \n",
    "    print(f'Hits@1: {hits1:.4f}')\n",
    "    print(f'Hits@3: {hits3:.4f}')\n",
    "    print(f'Hits@10: {hits10:.4f}')\n",
    "    print(f'MRR: {mrr:.4f}')\n",
    "    print(f'MR: {mr:.4f}')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c88296",
   "metadata": {},
   "source": [
    "# Knowledge Graph Embedding Experiments on Custom Datasets\n",
    "\n",
    "This notebook performs knowledge graph embedding experiments on custom datasets stored in the `data/skg/` folder. It replicates the experiments from the `wn_completion_experiments.ipynb` notebook but uses custom knowledge graphs instead of WordNet.\n",
    "\n",
    "The notebook includes experiments with four different knowledge graph embedding models:\n",
    "1. RotatE\n",
    "2. DistMult\n",
    "3. ComplEx\n",
    "4. TransE\n",
    "\n",
    "The main steps are:\n",
    "1. Loading custom triples from the dataset folders\n",
    "2. Running experiments with each model\n",
    "3. Comparing results across models\n",
    "4. Experimenting with multiple datasets\n",
    "5. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376f2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['shakespeare', 'politeness', 'olid', 'sarcasm', 'appdia', 'paradetox', 'gyafc', 'yelp', 'imdb', 'wnc']\n"
     ]
    }
   ],
   "source": [
    "# Get list of available datasets\n",
    "dataset_folders = [d for d in os.listdir('/home/bosa/skg/data/skg/') \n",
    "                 if os.path.isdir(os.path.join('/home/bosa/skg/data/skg/', d)) \n",
    "                 and os.path.exists(os.path.join('/home/bosa/skg/data/skg/', d, 'triples.tsv'))]\n",
    "print(f\"Available datasets: {dataset_folders}\")\n",
    "\n",
    "# Select a dataset to experiment with\n",
    "selected_dataset = 'imdb'  # Change this to experiment with different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195978aa",
   "metadata": {},
   "source": [
    "## Graph Creation Approach\n",
    "\n",
    "This notebook offers two approaches to create knowledge graphs for experiments:\n",
    "\n",
    "1. **Direct Graph Creation**: Using the `create_nx_graph` function from the `graph_creation` module to create NetworkX graph objects directly from the source data files (nodes, edges, etc.)\n",
    "\n",
    "2. **Triples File Loading**: Loading pre-generated triples from `triples.tsv` files in each dataset folder\n",
    "\n",
    "The notebook will attempt the direct graph creation first and fall back to triples file loading if there are any issues. You can also specify which method to use when creating the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create triples from NetworkX graph for PyKEEN\n",
    "def create_triples_from_graph(graph):\n",
    "    \"\"\"\n",
    "    Convert a NetworkX graph to a list of triples (head, relation, tail) for PyKEEN\n",
    "    \n",
    "    Args:\n",
    "        graph: NetworkX MultiGraph object\n",
    "        \n",
    "    Returns:\n",
    "        List of triples in the format [head, relation, tail]\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    \n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        edge_type = data.get('edge_type', 'default_edge')\n",
    "        triples.append([u, edge_type, v])\n",
    "        \n",
    "    return triples\n",
    "\n",
    "\n",
    "# Fix import issues with the graph_creation module\n",
    "import sys\n",
    "sys.path.append('/home/bosa/skg')\n",
    "\n",
    "# Import networkx explicitly\n",
    "import networkx as nx\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Create a wrapper function to run create_nx_graph with proper imports\n",
    "def create_nx_graph_wrapper(dataset_name, style_1_name, style_2_name):\n",
    "    \"\"\"\n",
    "    Wrapper to create NetworkX graph with proper imports\n",
    "    \n",
    "    This function handles the import issues in create_graph.py by\n",
    "    ensuring all required modules are in the correct path\n",
    "    \"\"\"\n",
    "    # Save current directory\n",
    "    original_dir = os.getcwd()\n",
    "    \n",
    "    try:\n",
    "        # Change to the graph_creation directory to make relative imports work\n",
    "        # os.chdir('/home/bosa/skg/graph_creation')\n",
    "        \n",
    "        # # Add graph_creation to the path\n",
    "        # if '/home/bosa/skg/graph_creation' not in sys.path:\n",
    "        #     sys.path.append('/home/bosa/skg/graph_creation')\n",
    "        \n",
    "        # # Now import modules directly from their location\n",
    "        from util.graph_creation.create_graph import create_nx_graph\n",
    "        \n",
    "        # Create and return the graph\n",
    "        return create_nx_graph(dataset_name, style_1_name, style_2_name)\n",
    "    \n",
    "    finally:\n",
    "        # Restore original directory\n",
    "        os.chdir(original_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d70746f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error checking modules: name 'create_graph' is not defined\n",
      "Please make sure all required modules are installed and properly imported\n"
     ]
    }
   ],
   "source": [
    "# Make sure networkx is imported\n",
    "import networkx as nx\n",
    "\n",
    "# Check if all necessary modules are available\n",
    "try:\n",
    "    print(f\"Imported create_graph module: {create_graph.__name__}\")\n",
    "    print(f\"Imported create_nodes module: {create_nodes.__name__}\")\n",
    "    print(f\"Imported create_edges module: {create_edges.__name__}\")\n",
    "    print(f\"All required modules are available\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking modules: {e}\")\n",
    "    print(\"Please make sure all required modules are installed and properly imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to use prepare_triples to generate triple files\n",
    "def create_triples_file(dataset_name):\n",
    "    \"\"\"\n",
    "    Use the prepare_triples module to create a triples.tsv file for the dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset (e.g., 'appdia', 'imdb', etc.)\n",
    "    \"\"\"\n",
    "    # Get style names from the pmi edge files\n",
    "    data_dir = f'/home/bosa/skg/data/skg/{dataset_name}'\n",
    "    pmi_files = [f for f in os.listdir(data_dir) if f.startswith('pmi_')]\n",
    "    \n",
    "    # Extract style names\n",
    "    style_names = [f.replace('pmi_', '').replace('.edges', '') for f in pmi_files]\n",
    "    if len(style_names) < 2:\n",
    "        raise ValueError(f\"Need at least 2 style names for dataset {dataset_name}, found {style_names}\")\n",
    "    \n",
    "    # Import prepare_triples\n",
    "    from graph_creation import prepare_triples\n",
    "    \n",
    "    # Use prepare_triples to create the triples file\n",
    "    print(f\"Creating triples for {dataset_name} with styles: {style_names[0]} and {style_names[1]}\")\n",
    "    prepare_triples.prepare_triples(dataset_name, style_names[0], style_names[1])\n",
    "    \n",
    "    # Verify the file was created\n",
    "    triples_path = f\"/home/bosa/skg/data/skg/{dataset_name}/triples.tsv\"\n",
    "    if os.path.exists(triples_path):\n",
    "        print(f\"Successfully created triples file at {triples_path}\")\n",
    "        # Count number of triples\n",
    "        with open(triples_path, 'r') as f:\n",
    "            num_lines = sum(1 for _ in f) - 1  # Subtract 1 for header\n",
    "        print(f\"Number of triples: {num_lines}\")\n",
    "    else:\n",
    "        print(f\"Failed to create triples file at {triples_path}\")\n",
    "\n",
    "# Uncomment to create triples file for a dataset if needed\n",
    "# create_triples_file(selected_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca7522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to choose the best approach for knowledge graph creation\n",
    "def create_knowledge_graph(dataset_name, method='direct', recreate_triples=False):\n",
    "    \"\"\"\n",
    "    Creates a knowledge graph for the given dataset using either direct graph creation\n",
    "    or loading from triples file\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset (e.g., 'appdia', 'imdb', etc.)\n",
    "        method: Method to use for graph creation ('direct' or 'triples')\n",
    "        recreate_triples: Whether to recreate the triples file if using 'triples' method\n",
    "        \n",
    "    Returns:\n",
    "        TriplesFactory object with the graph data\n",
    "    \"\"\"\n",
    "    if method == 'direct':\n",
    "        try:\n",
    "            print(f\"Attempting direct graph creation for {dataset_name}...\")\n",
    "            return create_and_prepare_graph(dataset_name), True\n",
    "        except Exception as e:\n",
    "            print(f\"Direct graph creation failed: {e}\")\n",
    "            print(\"Falling back to triples file method...\")\n",
    "            method = 'triples'\n",
    "    \n",
    "    if method == 'triples':\n",
    "        # Check if triples file exists and recreate if needed\n",
    "        triples_path = f\"/home/bosa/skg/data/skg/{dataset_name}/triples.tsv\"\n",
    "        if not os.path.exists(triples_path) or recreate_triples:\n",
    "            try:\n",
    "                print(f\"Creating triples file for {dataset_name}...\")\n",
    "                create_triples_file(dataset_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to create triples file: {e}\")\n",
    "                raise ValueError(f\"Cannot create knowledge graph for {dataset_name}: both methods failed\")\n",
    "        \n",
    "        # Load from triples file\n",
    "        print(f\"Loading from triples file for {dataset_name}...\")\n",
    "        return load_custom_dataset(dataset_name), False\n",
    "    \n",
    "    raise ValueError(f\"Invalid method: {method}. Use 'direct' or 'triples'.\")\n",
    "\n",
    "# Example usage:\n",
    "# graph_triples_factory, use_created_graph = create_knowledge_graph(selected_dataset, method='direct')\n",
    "# Or to force using triples file:\n",
    "# graph_triples_factory, use_created_graph = create_knowledge_graph(selected_dataset, method='triples', recreate_triples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create graph and prepare it for PyKEEN\n",
    "def create_and_prepare_graph(dataset_name):\n",
    "    \"\"\"\n",
    "    Create a NetworkX graph using create_nx_graph and prepare it for PyKEEN\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset (e.g., 'appdia', 'imdb', etc.)\n",
    "        \n",
    "    Returns:\n",
    "        TriplesFactory object with the graph data\n",
    "    \"\"\"\n",
    "    # Get style names from the pmi edge files\n",
    "    data_dir = f'/home/bosa/skg/data/skg/{dataset_name}'\n",
    "    pmi_files = [f for f in os.listdir(data_dir) if f.startswith('pmi_')]\n",
    "    \n",
    "    # Extract style names\n",
    "    style_names = [f.replace('pmi_', '').replace('.edges', '') for f in pmi_files]\n",
    "    if len(style_names) < 2:\n",
    "        raise ValueError(f\"Need at least 2 style names for dataset {dataset_name}, found {style_names}\")\n",
    "    \n",
    "    # Call our wrapper function instead of directly using create_nx_graph\n",
    "    print(f\"Creating graph for {dataset_name} with styles: {style_names[0]} and {style_names[1]}\")\n",
    "    graph = create_nx_graph_wrapper(dataset_name, style_names[0], style_names[1])\n",
    "    \n",
    "    # Get some statistics\n",
    "    print(f\"Graph Statistics:\")\n",
    "    print(f\"Number of nodes: {graph.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {graph.number_of_edges()}\")\n",
    "    \n",
    "    # Get edge types\n",
    "    edge_types = set()\n",
    "    for _, _, data in graph.edges(data=True):\n",
    "        if 'edge_type' in data:\n",
    "            edge_types.add(data['edge_type'])\n",
    "    print(f\"Edge types: {edge_types}\")\n",
    "    \n",
    "    # Convert to triples\n",
    "    triples = create_triples_from_graph(graph)\n",
    "    \n",
    "    # Create mappings for entities and relations\n",
    "    entity_to_id = {}\n",
    "    relation_to_id = {}\n",
    "    \n",
    "    # First pass to create mappings\n",
    "    for h, r, t in triples:\n",
    "        if h not in entity_to_id:\n",
    "            entity_to_id[h] = len(entity_to_id)\n",
    "        if t not in entity_to_id:\n",
    "            entity_to_id[t] = len(entity_to_id)\n",
    "        if r not in relation_to_id:\n",
    "            relation_to_id[r] = len(relation_to_id)\n",
    "    \n",
    "    # Second pass to create mapped triples\n",
    "    mapped_triples = []\n",
    "    for h, r, t in triples:\n",
    "        mapped_triples.append([entity_to_id[h], relation_to_id[r], entity_to_id[t]])\n",
    "    \n",
    "    # Create TriplesFactory\n",
    "    from pykeen.triples import TriplesFactory\n",
    "    return TriplesFactory(\n",
    "        mapped_triples=np.array(mapped_triples),\n",
    "        entity_to_id=entity_to_id,\n",
    "        relation_to_id=relation_to_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3dbcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating knowledge graph for appdia...\n",
      "Attempting direct graph creation for appdia...\n",
      "Creating graph for appdia with styles: offensive and non-offensive\n",
      "Direct graph creation failed: No module named 'nltk'\n",
      "Falling back to triples file method...\n",
      "Loading from triples file for appdia...\n",
      "\n",
      "Successfully created graph and converted to PyKEEN TriplesFactory\n",
      "Method used: Triples file loading\n",
      "Number of unique entities: 14880\n",
      "Number of unique relations: 6\n",
      "Number of triples: 24482\n"
     ]
    }
   ],
   "source": [
    "# Create a graph for the selected dataset\n",
    "try:\n",
    "    print(f\"Creating knowledge graph for {selected_dataset}...\")\n",
    "    # Use the unified approach - tries direct graph creation first, falls back to triples if needed\n",
    "    graph_triples_factory, use_created_graph = create_knowledge_graph(selected_dataset, method='direct')\n",
    "    \n",
    "    # Print info about the created graph\n",
    "    print(f\"\\nSuccessfully created graph and converted to PyKEEN TriplesFactory\")\n",
    "    print(f\"Method used: {'Direct graph creation' if use_created_graph else 'Triples file loading'}\")\n",
    "    print(f\"Number of unique entities: {len(graph_triples_factory.entity_to_id)}\")\n",
    "    print(f\"Number of unique relations: {len(graph_triples_factory.relation_to_id)}\")\n",
    "    print(f\"Number of triples: {graph_triples_factory.num_triples}\")\n",
    "    \n",
    "    # Set flag for model experiments\n",
    "    graph_factory_available = True\n",
    "    \n",
    "    # Optionally visualize a small sample of the graph\n",
    "    # Uncomment to visualize\n",
    "    # if use_created_graph:\n",
    "    #     visualize_graph_sample(selected_dataset, sample_size=30)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating graph: {e}\")\n",
    "    print(\"Will fall back to loading triples from triples.tsv file in each experiment\")\n",
    "    graph_factory_available = False\n",
    "    graph_triples_factory = None\n",
    "    use_created_graph = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccfcf0",
   "metadata": {},
   "source": [
    "# Graph Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd83b8",
   "metadata": {},
   "source": [
    "# RotatE Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60323c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using automatically assigned random_state=1915716316\n",
      "No random seed is specified. This may lead to non-reproducible results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with RotatE on appdia dataset\n",
      "Loading triples from triples.tsv file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No random seed is specified. Setting to 1768047711.\n",
      "Training epochs on cuda:0: 100%|██████████| 200/200 [00:53<00:00,  3.73epoch/s, loss=0.00266, prev_loss=0.00222]\n",
      "Evaluating on cuda:0: 100%|██████████| 4.90k/4.90k [00:03<00:00, 1.39ktriple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 3.56s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@1: 0.2614\n",
      "Hits@3: 0.3317\n",
      "Hits@10: 0.4015\n",
      "MRR: 0.3109\n",
      "MR: 1588.3792\n"
     ]
    }
   ],
   "source": [
    "# Run experiment with RotatE model\n",
    "rotate_result = run_experiment(\n",
    "    dataset_name=selected_dataset,\n",
    "    model_class=RotatE,\n",
    "    embedding_dim=128,  \n",
    "    num_epochs=200,\n",
    "    batch_size=512,\n",
    "    learning_rate=0.01,\n",
    "    use_graph_factory=use_created_graph,\n",
    "    graph_factory=graph_triples_factory if use_created_graph else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044f42d",
   "metadata": {},
   "source": [
    "# DistMult Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d83e81aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:using automatically assigned random_state=1326928479\n",
      "INFO:pykeen.triples.splitting:done splitting triples to groups of sizes [5264, 4897]\n",
      "WARNING:pykeen.models.base:No random seed is specified. This may lead to non-reproducible results.\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 1892241268.\n",
      "INFO:pykeen.pipeline.api:Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with DistMult on appdia dataset\n",
      "Loading triples from triples.tsv file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cuda:0: 100%|██████████| 200/200 [00:55<00:00,  3.59epoch/s, loss=0.563, prev_loss=0.573]\n",
      "Evaluating on cuda:0: 100%|██████████| 4.90k/4.90k [00:00<00:00, 5.37ktriple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.96s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@1: 0.1095\n",
      "Hits@3: 0.1507\n",
      "Hits@10: 0.2116\n",
      "MRR: 0.1443\n",
      "MR: 3912.2021\n"
     ]
    }
   ],
   "source": [
    "# Run experiment with DistMult model\n",
    "distmult_result = run_experiment(\n",
    "    dataset_name=selected_dataset,\n",
    "    model_class=DistMult,\n",
    "    embedding_dim=128,\n",
    "    num_epochs=200,\n",
    "    batch_size=512,\n",
    "    learning_rate=0.01,\n",
    "    use_graph_factory=use_created_graph,\n",
    "    graph_factory=graph_triples_factory if use_created_graph else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450790c4",
   "metadata": {},
   "source": [
    "# ComplEx Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544cce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:using automatically assigned random_state=53410780\n",
      "INFO:pykeen.triples.splitting:done splitting triples to groups of sizes [5264, 4897]\n",
      "WARNING:pykeen.models.base:No random seed is specified. This may lead to non-reproducible results.\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 1347430660.\n",
      "INFO:pykeen.pipeline.api:Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with ComplEx on appdia dataset\n",
      "Loading triples from triples.tsv file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cuda:0:  15%|█▌        | 30/200 [00:10<01:01,  2.74epoch/s, loss=2.94, prev_loss=3.01]"
     ]
    }
   ],
   "source": [
    "# Run experiment with ComplEx model\n",
    "complex_result = run_experiment(\n",
    "    dataset_name=selected_dataset,\n",
    "    model_class=ComplEx,\n",
    "    embedding_dim=128,\n",
    "    num_epochs=200,\n",
    "    batch_size=512,\n",
    "    learning_rate=0.01,\n",
    "    use_graph_factory=use_created_graph,\n",
    "    graph_factory=graph_triples_factory if use_created_graph else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e91679",
   "metadata": {},
   "source": [
    "# TransE Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d63dd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:using automatically assigned random_state=1552704614\n",
      "INFO:pykeen.triples.splitting:done splitting triples to groups of sizes [5264, 4897]\n",
      "WARNING:pykeen.models.base:No random seed is specified. This may lead to non-reproducible results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 1939368073.\n",
      "INFO:pykeen.pipeline.api:Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with TransE on appdia dataset\n",
      "Loading triples from triples.tsv file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cuda:0: 100%|██████████| 100/100 [00:25<00:00,  3.90epoch/s, loss=0.0057, prev_loss=0.00575]\n",
      "Evaluating on cuda:0: 100%|██████████| 4.90k/4.90k [00:00<00:00, 5.44ktriple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.95s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@1: 0.0000\n",
      "Hits@3: 0.0379\n",
      "Hits@10: 0.0854\n",
      "MRR: 0.0316\n",
      "MR: 2320.5002\n"
     ]
    }
   ],
   "source": [
    "# Run experiment with TransE model\n",
    "transe_result = run_experiment(\n",
    "    dataset_name=selected_dataset,\n",
    "    model_class=TransE,\n",
    "    embedding_dim=128,\n",
    "    num_epochs=200,\n",
    "    batch_size=512,\n",
    "    learning_rate=0.01,\n",
    "    use_graph_factory=use_created_graph,\n",
    "    graph_factory=graph_triples_factory if use_created_graph else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9a78f",
   "metadata": {},
   "source": [
    "# Comparing Results Across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7d066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HITS_AT_1:\n",
      "RotatE: 0.2493\n",
      "DistMult: 0.0711\n",
      "ComplEx: 0.0002\n",
      "TransE: 0.0000\n",
      "\n",
      "HITS_AT_3:\n",
      "RotatE: 0.3182\n",
      "DistMult: 0.0984\n",
      "TransE: 0.0379\n",
      "ComplEx: 0.0006\n",
      "\n",
      "HITS_AT_10:\n",
      "RotatE: 0.3841\n",
      "DistMult: 0.1327\n",
      "TransE: 0.0854\n",
      "ComplEx: 0.0015\n",
      "\n",
      "MEAN_RECIPROCAL_RANK:\n",
      "RotatE: 0.2979\n",
      "DistMult: 0.0918\n",
      "TransE: 0.0316\n",
      "ComplEx: 0.0011\n",
      "\n",
      "MEAN_RANK:\n",
      "RotatE: 1338.8655\n",
      "TransE: 2320.5002\n",
      "DistMult: 5336.5811\n",
      "ComplEx: 7393.7578\n"
     ]
    }
   ],
   "source": [
    "# Function to collect metrics from all models\n",
    "def compare_models(results_dict):\n",
    "    metrics = ['hits_at_1', 'hits_at_3', 'hits_at_10', 'mean_reciprocal_rank', 'mean_rank']\n",
    "    comparison = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        comparison[metric] = []\n",
    "        for model_name, result in results_dict.items():\n",
    "            comparison[metric].append((model_name, result.get_metric(metric)))\n",
    "    \n",
    "    # Print comparison\n",
    "    for metric in metrics:\n",
    "        print(f\"\\n{metric.upper()}:\")\n",
    "        for model_name, value in sorted(comparison[metric], key=lambda x: x[1], reverse=True if metric != 'mean_rank' else False):\n",
    "            print(f\"{model_name}: {value:.4f}\")\n",
    "\n",
    "# Collect all results\n",
    "all_results = {\n",
    "    'RotatE': rotate_result,\n",
    "    'DistMult': distmult_result,\n",
    "    'ComplEx': complex_result,\n",
    "    'TransE': transe_result\n",
    "}\n",
    "\n",
    "# Compare models\n",
    "compare_models(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac6f3d",
   "metadata": {},
   "source": [
    "# Experimenting with Multiple Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d1f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Select datasets to experiment with\\nselected_datasets = ['appdia', 'imdb', 'olid']  # Add more or change as needed\\n\\n# Run experiments with the best performing model (based on previous comparison)\\nbest_model = ComplEx  # Change this to the best model from your comparison\\n\\n# Run multi-dataset experiments\\ndataset_results = run_multi_dataset_experiments(\\n    datasets=selected_datasets,\\n    model_class=best_model,\\n    use_created_graphs=True,  # Set to False to use triples.tsv files instead\\n    embedding_dim=128,\\n    num_epochs=100,\\n    batch_size=512,\\n    learning_rate=0.01\\n)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to run experiments on multiple datasets with the best performing model\n",
    "def run_multi_dataset_experiments(datasets, model_class, use_created_graphs=True, **kwargs):\n",
    "    results = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        print(f\"\\n{'='*50}\\nRunning experiments on {dataset} dataset\\n{'='*50}\")\n",
    "        \n",
    "        # Try to create graph first\n",
    "        if use_created_graphs:\n",
    "            try:\n",
    "                graph_factory = create_and_prepare_graph(dataset)\n",
    "                use_graph = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating graph for {dataset}: {e}\")\n",
    "                print(\"Will fall back to loading triples from triples.tsv file\")\n",
    "                graph_factory = None\n",
    "                use_graph = False\n",
    "        else:\n",
    "            graph_factory = None\n",
    "            use_graph = False\n",
    "        \n",
    "        # Run experiment\n",
    "        results[dataset] = run_experiment(\n",
    "            dataset_name=dataset, \n",
    "            model_class=model_class, \n",
    "            use_graph_factory=use_graph,\n",
    "            graph_factory=graph_factory,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Uncomment and run the following to experiment with multiple datasets\n",
    "'''\n",
    "# Select datasets to experiment with\n",
    "selected_datasets = ['appdia', 'imdb', 'olid']  # Add more or change as needed\n",
    "\n",
    "# Run experiments with the best performing model (based on previous comparison)\n",
    "best_model = ComplEx  # Change this to the best model from your comparison\n",
    "\n",
    "# Run multi-dataset experiments\n",
    "dataset_results = run_multi_dataset_experiments(\n",
    "    datasets=selected_datasets,\n",
    "    model_class=best_model,\n",
    "    use_created_graphs=True,  # Set to False to use triples.tsv files instead\n",
    "    embedding_dim=128,\n",
    "    num_epochs=200,\n",
    "    batch_size=512,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec0f22",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b90c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Define hyperparameters to tune\\nembedding_dims = [100, 500, 1024]\\nlearning_rates = [0.001, 0.01, 0.1]\\n\\n# Select best model and dataset\\nbest_model = ComplEx  # Change based on your earlier results\\ntuning_dataset = selected_dataset\\n\\n# Run hyperparameter tuning\\nhyperparameter_results = run_hyperparameter_tuning(\\n    dataset_name=tuning_dataset,\\n    model_class=best_model,\\n    embedding_dims=embedding_dims,\\n    learning_rates=learning_rates,\\n    use_created_graph=use_created_graph,\\n    graph_factory=graph_triples_factory if use_created_graph else None\\n)\\n\\n# Sort by MRR (or another metric of your choice) to find best hyperparameters\\nhyperparameter_results.sort_values('mrr', ascending=False)\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to run hyperparameter tuning experiments\n",
    "def run_hyperparameter_tuning(dataset_name, model_class, embedding_dims, learning_rates, use_created_graph=True, graph_factory=None, num_epochs=200, batch_size=512):\n",
    "    results = []\n",
    "    \n",
    "    for embedding_dim in embedding_dims:\n",
    "        for lr in learning_rates:\n",
    "            print(f\"\\n{'='*50}\\nTesting with embedding_dim={embedding_dim}, lr={lr}\\n{'='*50}\")\n",
    "            \n",
    "            result = run_experiment(\n",
    "                dataset_name=dataset_name,\n",
    "                model_class=model_class,\n",
    "                embedding_dim=embedding_dim,\n",
    "                num_epochs=num_epochs,\n",
    "                batch_size=batch_size,\n",
    "                learning_rate=lr,\n",
    "                use_graph_factory=use_created_graph,\n",
    "                graph_factory=graph_factory\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'embedding_dim': embedding_dim,\n",
    "                'learning_rate': lr,\n",
    "                'hits@1': result.get_metric('hits_at_1'),\n",
    "                'hits@3': result.get_metric('hits_at_3'),\n",
    "                'hits@10': result.get_metric('hits_at_10'),\n",
    "                'mrr': result.get_metric('mean_reciprocal_rank'),\n",
    "                'mr': result.get_metric('mean_rank')\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame for easy analysis\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example: Uncomment and run the following to perform hyperparameter tuning\n",
    "'''\n",
    "# Define hyperparameters to tune\n",
    "embedding_dims = [100, 500, 1024]\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Select best model and dataset\n",
    "best_model = ComplEx  # Change based on your earlier results\n",
    "tuning_dataset = selected_dataset\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "hyperparameter_results = run_hyperparameter_tuning(\n",
    "    dataset_name=tuning_dataset,\n",
    "    model_class=best_model,\n",
    "    embedding_dims=embedding_dims,\n",
    "    learning_rates=learning_rates,\n",
    "    use_created_graph=use_created_graph,\n",
    "    graph_factory=graph_triples_factory if use_created_graph else None\n",
    ")\n",
    "\n",
    "# Sort by MRR (or another metric of your choice) to find best hyperparameters\n",
    "hyperparameter_results.sort_values('mrr', ascending=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7fa174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize a subset of the graph\n",
    "def visualize_graph_sample(dataset_name, sample_size=100):\n",
    "    \"\"\"\n",
    "    Create and visualize a sample of the graph\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset\n",
    "        sample_size: Number of nodes to sample\n",
    "    \"\"\"\n",
    "    # Get style names\n",
    "    data_dir = f'/home/bosa/skg/data/skg/{dataset_name}'\n",
    "    pmi_files = [f for f in os.listdir(data_dir) if f.startswith('pmi_')]\n",
    "    style_names = [f.replace('pmi_', '').replace('.edges', '') for f in pmi_files]\n",
    "    \n",
    "    # Create graph using our wrapper function\n",
    "    graph = create_nx_graph_wrapper(dataset_name, style_names[0], style_names[1])\n",
    "    \n",
    "    # Sample nodes\n",
    "    nodes = list(graph.nodes())\n",
    "    if len(nodes) > sample_size:\n",
    "        sampled_nodes = np.random.choice(nodes, sample_size, replace=False)\n",
    "        subgraph = graph.subgraph(sampled_nodes)\n",
    "    else:\n",
    "        subgraph = graph\n",
    "    \n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Set edge colors based on edge type\n",
    "    edge_colors = []\n",
    "    edge_types = set()\n",
    "    for _, _, data in subgraph.edges(data=True):\n",
    "        edge_type = data.get('edge_type', 'default_edge')\n",
    "        edge_types.add(edge_type)\n",
    "    \n",
    "    # Create a color map\n",
    "    edge_type_to_color = {}\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink']\n",
    "    for i, edge_type in enumerate(edge_types):\n",
    "        edge_type_to_color[edge_type] = colors[i % len(colors)]\n",
    "    \n",
    "    # Get edge colors\n",
    "    for _, _, data in subgraph.edges(data=True):\n",
    "        edge_type = data.get('edge_type', 'default_edge')\n",
    "        edge_colors.append(edge_type_to_color[edge_type])\n",
    "    \n",
    "    # Draw the graph\n",
    "    pos = nx.spring_layout(subgraph, seed=42)  # Position nodes using Fruchterman-Reingold\n",
    "    nx.draw(subgraph, pos, with_labels=True, node_size=300, node_color='skyblue', \n",
    "            font_size=8, font_weight='bold', edge_color=edge_colors, width=1.5, alpha=0.7)\n",
    "    \n",
    "    # Create legend\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], color=color, lw=2, label=edge_type)\n",
    "        for edge_type, color in edge_type_to_color.items()\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, title=\"Edge Types\")\n",
    "    \n",
    "    plt.title(f\"Sample of {dataset_name} Knowledge Graph\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to visualize a sample of the graph\n",
    "# visualize_graph_sample(selected_dataset, sample_size=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
